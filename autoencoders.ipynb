{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Functions\n",
    "Here we define the functions used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function build the auto encoder model\n",
    "def build_autoencoder(data, num_hidden_layers = 2, num_nodes = 16, act = 'relu'):\n",
    "\n",
    "    model=Sequential()\n",
    "    \n",
    "    input_layer= Input(shape=(data.shape[1],))\n",
    "    model.add(input_layer)\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(units = num_nodes, activation=act))\n",
    "\n",
    "            \n",
    "    model.add(Dense(units=data.shape[1], activation=act))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss= 'mean_squared_error', metrics=['mse']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>all_year_avail</th>\n",
       "      <th>low_avail</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbourhood_Williamsburg</th>\n",
       "      <th>neighbourhood_Willowbrook</th>\n",
       "      <th>neighbourhood_Windsor Terrace</th>\n",
       "      <th>neighbourhood_Woodhaven</th>\n",
       "      <th>neighbourhood_Woodlawn</th>\n",
       "      <th>neighbourhood_Woodrow</th>\n",
       "      <th>neighbourhood_Woodside</th>\n",
       "      <th>room_type_Entire home/apt</th>\n",
       "      <th>room_type_Private room</th>\n",
       "      <th>room_type_Shared room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2762</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>5.420535</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2976</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.80902</td>\n",
       "      <td>-73.94190</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.68514</td>\n",
       "      <td>-73.95976</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>3021</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.79851</td>\n",
       "      <td>-73.94399</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2793</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude     price  minimum_nights  number_of_reviews  \\\n",
       "0  40.64749  -73.97237  5.010635               1                  9   \n",
       "1  40.75362  -73.98377  5.420535               1                 45   \n",
       "2  40.80902  -73.94190  5.017280               3                  0   \n",
       "3  40.68514  -73.95976  4.499810               1                270   \n",
       "4  40.79851  -73.94399  4.394449              10                  9   \n",
       "\n",
       "   last_review  reviews_per_month  calculated_host_listings_count  \\\n",
       "0         2762               0.21                               6   \n",
       "1         2976               0.38                               2   \n",
       "2            0               0.00                               1   \n",
       "3         3021               4.64                               1   \n",
       "4         2793               0.10                               1   \n",
       "\n",
       "   all_year_avail  low_avail  ...  neighbourhood_Williamsburg  \\\n",
       "0            True      False  ...                           0   \n",
       "1            True      False  ...                           0   \n",
       "2            True      False  ...                           0   \n",
       "3           False      False  ...                           0   \n",
       "4           False       True  ...                           0   \n",
       "\n",
       "   neighbourhood_Willowbrook  neighbourhood_Windsor Terrace  \\\n",
       "0                          0                              0   \n",
       "1                          0                              0   \n",
       "2                          0                              0   \n",
       "3                          0                              0   \n",
       "4                          0                              0   \n",
       "\n",
       "   neighbourhood_Woodhaven  neighbourhood_Woodlawn  neighbourhood_Woodrow  \\\n",
       "0                        0                       0                      0   \n",
       "1                        0                       0                      0   \n",
       "2                        0                       0                      0   \n",
       "3                        0                       0                      0   \n",
       "4                        0                       0                      0   \n",
       "\n",
       "   neighbourhood_Woodside  room_type_Entire home/apt  room_type_Private room  \\\n",
       "0                       0                          0                       1   \n",
       "1                       0                          1                       0   \n",
       "2                       0                          0                       1   \n",
       "3                       0                          1                       0   \n",
       "4                       0                          1                       0   \n",
       "\n",
       "   room_type_Shared room  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 240 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('input/processed_data_nyc.csv', index_col = 0)\n",
    "# numerical_data = pd.read_csv('input/processed_data_nyc_numerical.csv',  index_col = 0)\n",
    "# categorical_data = pd.read_csv('input/processed_data_nyc_categorical.csv',  index_col = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to divide the data into categorical and numerical. We will attempt to get a new representation of the numerical data using the autoencoder. We will compare the new representation to the old one to see which one produces better performance using the same classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.price\n",
    "data = data.drop(['price'], axis=1)\n",
    "\n",
    "# Converting to numpy arrays\n",
    "X = np.asarray(data).astype(np.float32)\n",
    "y = np.asarray(y).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting to train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset: (39014, 239)\n",
      "Testing Dataset: (9754, 239)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training Dataset: {}\".format(X_train.shape))\n",
    "print(\"Testing Dataset: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Testing Autoencoders\n",
    "In this section, I will test various architectures for autoencoders and judge based on Mean-Squared Error (MSE) as well as the performance of the three classifiers with and without autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareClasifiers (ae_model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Classifiers\n",
    "    randomForest_final = RandomForestRegressor(n_estimators=50)\n",
    "    ridge_final = Ridge(alpha=5)\n",
    "    huber_final = HuberRegressor(alpha=10, epsilon=3)\n",
    "    \n",
    "    #New representations:\n",
    "    X_train2 = ae_model.predict(X_train)\n",
    "    X_test2 = ae_model.predict(X_test)\n",
    "        \n",
    "    rf_result_final = []\n",
    "    ridge_result_final = []\n",
    "    huber_result_final = []\n",
    "    \n",
    "    # Random Forest\n",
    "    randomForest_final.fit(X_train,y_train)\n",
    "    rf_y = randomForest_final.predict(X_test)\n",
    "    rf_result = mean_squared_error(y_test, rf_y)\n",
    "\n",
    "    randomForest_final.fit(X_train2,y_train)\n",
    "    rf_y2 = randomForest_final.predict(X_test2)\n",
    "    rf_result2 = mean_squared_error(y_test, rf_y2)\n",
    "\n",
    "    print(\"Random Forest: {}\".format(rf_result))\n",
    "    print(\"Random Forest with Autoencoder: {}\".format(rf_result2))\n",
    "    rf_result_final.append(rf_result)\n",
    "    rf_result_final.append(rf_result2)\n",
    "    rf_result_final.append(rf_result-rf_result2)\n",
    "    \n",
    "    # Ridge\n",
    "    ridge_final.fit(X_train,y_train)\n",
    "    ridge_y = ridge_final.predict(X_test)\n",
    "    ridge_result = mean_squared_error(y_test, ridge_y)\n",
    "\n",
    "    ridge_final.fit(X_train2,y_train)\n",
    "    ridge_y2 = ridge_final.predict(X_test2)\n",
    "    ridge_result2 = mean_squared_error(y_test, ridge_y2)\n",
    "\n",
    "    print(\"Ridge : {}\".format(ridge_result))\n",
    "    print(\"Ridge with Autoencoder: {}\".format(ridge_result2))\n",
    "    ridge_result_final.append(ridge_result)\n",
    "    ridge_result_final.append(ridge_result2)\n",
    "    ridge_result_final.append(ridge_result-ridge_result2)\n",
    "\n",
    "    # Huber\n",
    "    huber_final.fit(X_train,y_train)\n",
    "    huber_y = huber_final.predict(X_test)\n",
    "    huber_result = mean_squared_error(y_test,huber_y)\n",
    "\n",
    "    huber_final.fit(X_train2,y_train)\n",
    "    huber_y2 = huber_final.predict(X_test2)\n",
    "    huber_result2 = mean_squared_error(y_test,huber_y2)\n",
    "\n",
    "    print(\"Huber: {}\".format(huber_result))\n",
    "    print(\"Huber with Autoencoder: {}\".format(huber_result2))\n",
    "    huber_result_final.append(huber_result)\n",
    "    huber_result_final.append(huber_result2)\n",
    "    huber_result_final.append(huber_result-ridge_result2)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'RF': rf_result_final,\n",
    "        'Ridge': ridge_result_final,\n",
    "        'Huber': huber_result_final,\n",
    "\n",
    "        \n",
    "    },\n",
    "    index = ['Before','After','Difference']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Test 1: Layers and Nodes\n",
    "In the first test, I will cross-validate for different numbers of layers and nodes. I will use 100 epochs with 512 for batch size. Once an ideal architecture is chosen, I will test the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-16-Epochs=30-TIME=1586823117\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 4s 123us/sample - loss: 3.6280 - mse: 3.6280 - val_loss: 2.1644 - val_mse: 2.1644\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 2s 60us/sample - loss: 0.9435 - mse: 0.9435 - val_loss: 0.1791 - val_mse: 0.1791\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 2s 66us/sample - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0706 - val_mse: 0.0706\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 3s 81us/sample - loss: 0.0488 - mse: 0.0488 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0228 - val_mse: 0.0228\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 2s 63us/sample - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 1s 41us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 1s 33us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 4s 127us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 76us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 67us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 75us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 2s 68us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 2s 78us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 2s 67us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 2s 59us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 2s 65us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 4s 143us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 2s 77us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 2s 71us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 2s 71us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 2s 73us/sample - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 2s 76us/sample - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "1-64-Epochs=30-TIME=1586823183\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 3.1748 - mse: 3.1748 - val_loss: 0.4635 - val_mse: 0.4635\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.0865 - mse: 0.0865 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 1s 32us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 4s 138us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 3s 88us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 3s 100us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 2s 75us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 2s 74us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 4s 128us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 3s 100us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 3s 88us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 2s 70us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 82us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 4s 138us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 3s 100us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 64us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 2s 59us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 2s 65us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 2s 65us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "1-128-Epochs=30-TIME=1586823261\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 5s 162us/sample - loss: 1.4462 - mse: 1.4462 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 3s 93us/sample - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 3s 93us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 2s 63us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 3s 85us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 2s 72us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 5s 163us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 2s 74us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 2s 68us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 68us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 53us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 2s 59us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 1s 48us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 1s 35us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 1s 31us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 3s 86us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 3s 95us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138 loss: 0.0137 - mse: 0\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 3s 83us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 2s 73us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 3s 99us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 3s 85us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 83us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "1-512-Epochs=30-TIME=1586823335\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.7986 - mse: 0.7986 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 3s 81us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 3s 104us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 7s 225us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 5s 169us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 5s 147us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 4s 132us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 6s 195us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 5s 165us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 4s 125us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 3s 108us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 112us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 3s 90us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 6s 187us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 4s 135us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 4s 125us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 4s 120us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 4s 121us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 6s 196us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 4s 140us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 4s 144us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 4s 118us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 104us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 3s 104us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 6s 178us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 4s 118us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 3s 111us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 3s 107us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 4s 123us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 3s 108us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "1-1024-Epochs=30-TIME=1586823459\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.7957 - mse: 0.7957 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 7s 217us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 5s 154us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 4s 139us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 4s 138us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 4s 114us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 2s 76us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 2s 77us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 3s 81us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 3s 106us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 4s 113us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 3s 111us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 3s 102us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 3s 106us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 3s 102us/sample - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 107us/sample - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 3s 99us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 102us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 3s 95us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 3s 100us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 3s 93us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 4s 119us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 4s 119us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 4s 116us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "2-16-Epochs=30-TIME=1586823560\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 3.9904 - mse: 3.9904 - val_loss: 1.5669 - val_mse: 1.5669\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.2759 - mse: 0.2759 - val_loss: 0.0790 - val_mse: 0.0790\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 2s 49us/sample - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 1s 48us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 1s 41us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 1s 41us/sample - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 1s 33us/sample - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 1s 47us/sample - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 1s 47us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 2s 50us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 1s 32us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 1s 42us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 1s 46us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 1s 35us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 1s 36us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 1s 36us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 1s 36us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 1s 30us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 1s 33us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 1s 33us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 1s 36us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "2-64-Epochs=30-TIME=1586823600\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 2.3683 - mse: 2.3683 - val_loss: 0.0809 - val_mse: 0.0809\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0234 - val_mse: 0.0234\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 1s 37us/sample - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 1s 46us/sample - loss: 0.0219 - mse: 0.0219 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 1s 47us/sample - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 1s 42us/sample - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 1s 47us/sample - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 1s 43us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 1s 41us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 1s 46us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 1s 48us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 2s 50us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 2s 50us/sample - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 1s 45us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 1s 44us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 1s 48us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 1s 42us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 1s 42us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 1s 42us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 1s 41us/sample - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "2-128-Epochs=30-TIME=1586823643\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 2s 68us/sample - loss: 1.1132 - mse: 1.1132 - val_loss: 0.0193 - val_mse: 0.0193\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 2s 61us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 53us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 2s 58us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 2s 59us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 2s 61us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 64us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 2s 60us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 2s 50us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 2s 53us/sample - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 2s 60us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 67us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 2s 70us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 2s 65us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 2s 58us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "2-512-Epochs=30-TIME=1586823697\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 4s 129us/sample - loss: 0.3124 - mse: 0.3124 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 4s 117us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 3s 112us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 4s 118us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 3s 107us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 4s 114us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 3s 110us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 4s 113us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 3s 107us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 3s 104us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 3s 106us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 112us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 4s 116us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 4s 126us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 4s 115us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 4s 117us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 3s 112us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 4s 114us/sample - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 4s 120us/sample - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 111us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 4s 114us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 4s 119us/sample - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 4s 113us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 3s 111us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 4s 115us/sample - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 3s 110us/sample - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 4s 115us/sample - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 4s 116us/sample - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 4s 119us/sample - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 3s 112us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "2-1024-Epochs=30-TIME=1586823804\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 15s 473us/sample - loss: 0.3857 - mse: 0.3857 - val_loss: 0.1643 - val_mse: 0.1643\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 11s 348us/sample - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1642 - val_mse: 0.1642\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 13s 411us/sample - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1643 - val_mse: 0.1643\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 13s 405us/sample - loss: 0.1541 - mse: 0.1541 - val_loss: 0.1641 - val_mse: 0.1641\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 12s 377us/sample - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1640 - val_mse: 0.1640\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 13s 413us/sample - loss: 0.1559 - mse: 0.1559 - val_loss: 0.1641 - val_mse: 0.1641\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 11s 365us/sample - loss: 0.1553 - mse: 0.1553 - val_loss: 0.1639 - val_mse: 0.1639\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 13s 402us/sample - loss: 0.1545 - mse: 0.1545 - val_loss: 0.1651 - val_mse: 0.1651\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 11s 353us/sample - loss: 0.1545 - mse: 0.1545 - val_loss: 0.1641 - val_mse: 0.1641\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 12s 384us/sample - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1646 - val_mse: 0.1646\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 11s 366us/sample - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1644 - val_mse: 0.1644\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 13s 405us/sample - loss: 0.1537 - mse: 0.1537 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 11s 359us/sample - loss: 0.1362 - mse: 0.1362 - val_loss: 0.1465 - val_mse: 0.1465\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 14s 434us/sample - loss: 0.1359 - mse: 0.1359 - val_loss: 0.1458 - val_mse: 0.1458\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 12s 371us/sample - loss: 0.1362 - mse: 0.1362 - val_loss: 0.1461 - val_mse: 0.1461\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 13s 408us/sample - loss: 0.1360 - mse: 0.1360 - val_loss: 0.1458 - val_mse: 0.1458\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 11s 354us/sample - loss: 0.1356 - mse: 0.1356 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 12s 388us/sample - loss: 0.1384 - mse: 0.1384 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 11s 337us/sample - loss: 0.1377 - mse: 0.1377 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 13s 410us/sample - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1456 - val_mse: 0.1456\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 11s 339us/sample - loss: 0.1357 - mse: 0.1357 - val_loss: 0.1459 - val_mse: 0.1459\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 13s 401us/sample - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1465 - val_mse: 0.1465\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 11s 337us/sample - loss: 0.1359 - mse: 0.1359 - val_loss: 0.1460 - val_mse: 0.1460\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 12s 390us/sample - loss: 0.1590 - mse: 0.1590 - val_loss: 0.1616 - val_mse: 0.1616\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 11s 338us/sample - loss: 0.1382 - mse: 0.1382 - val_loss: 0.1459 - val_mse: 0.1459\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 13s 401us/sample - loss: 0.1357 - mse: 0.1357 - val_loss: 0.1462 - val_mse: 0.1462\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 11s 363us/sample - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1459 - val_mse: 0.1459\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 10s 318us/sample - loss: 0.1357 - mse: 0.1357 - val_loss: 0.1460 - val_mse: 0.1460\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 11s 354us/sample - loss: 0.1357 - mse: 0.1357 - val_loss: 0.1459 - val_mse: 0.1459\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 9s 295us/sample - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1459 - val_mse: 0.1459\n",
      "3-16-Epochs=30-TIME=1586824157\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 3.0573 - mse: 3.0573 - val_loss: 0.1913 - val_mse: 0.1913\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 1s 35us/sample - loss: 0.1325 - mse: 0.1325 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 4s 140us/sample - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 2s 66us/sample - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 2s 61us/sample - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 2s 49us/sample - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 2s 50us/sample - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 1s 33us/sample - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 4s 138us/sample - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 65us/sample - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 2s 67us/sample - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 2s 66us/sample - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 80us/sample - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 2s 48us/sample - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 63us/sample - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 2s 66us/sample - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 4s 142us/sample - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 4s 119us/sample - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 3s 106us/sample - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 86us/sample - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "3-64-Epochs=30-TIME=1586824224\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 4s 137us/sample - loss: 1.7348 - mse: 1.7348 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0188 - val_mse: 0.0188\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 5s 157us/sample - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 3s 108us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0163 - val_mse: 0.0163\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 3s 101us/sample - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 3s 83us/sample - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 3s 87us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 3s 86us/sample - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 5s 149us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 3s 106us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 103us/sample - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 4s 116us/sample - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 3s 108us/sample - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 76us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 70us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 53us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 5s 164us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 95us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 3s 82us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 2s 68us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 90us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 76us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 2s 67us/sample - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 5s 149us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 3s 88us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 3s 82us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 81us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 2s 80us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "3-128-Epochs=30-TIME=1586824315\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 4s 118us/sample - loss: 0.6613 - mse: 0.6613 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 3s 85us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 2s 75us/sample - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 5s 172us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 3s 99us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 4s 114us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 3s 105us/sample - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 3s 90us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 2s 69us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 6s 179us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 95us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 4s 117us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 3s 106us/sample - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 3s 93us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 3s 94us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 3s 87us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 5s 172us/sample - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 3s 109us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 4s 115us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 3s 110us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 3s 108us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 89us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 5s 160us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 4s 118us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 4s 117us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 3s 102us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 3s 87us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "3-512-Epochs=30-TIME=1586824416\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 9s 294us/sample - loss: 0.1652 - mse: 0.1652 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 8s 256us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 7s 234us/sample - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 10s 304us/sample - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 8s 249us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 7s 209us/sample - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 10s 319us/sample - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 8s 252us/sample - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 6s 206us/sample - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 5s 152us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 8s 264us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 7s 226us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 7s 210us/sample - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 4s 129us/sample - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 9s 281us/sample - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 7s 226us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 7s 211us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 4s 141us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 4s 130us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 8s 256us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 7s 238us/sample - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 7s 214us/sample - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 9s 300us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 8s 252us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 6s 188us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 6s 192us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 8s 249us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 7s 216us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0199 - val_mse: 0.0199\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 6s 189us/sample - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 7s 231us/sample - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "3-1024-Epochs=30-TIME=1586824629\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 18s 572us/sample - loss: 0.3610 - mse: 0.3610 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 19s 622us/sample - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 19s 596us/sample - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 18s 570us/sample - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 17s 545us/sample - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 19s 607us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 19s 609us/sample - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 17s 551us/sample - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 18s 584us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 17s 539us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 19s 623us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 19s 604us/sample - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 19s 619us/sample - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 16s 524us/sample - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 19s 595us/sample - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 18s 591us/sample - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 19s 607us/sample - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0295 - val_mse: 0.0295\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 18s 591us/sample - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 17s 550us/sample - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 20s 625us/sample - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 18s 584us/sample - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 19s 598us/sample - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 18s 592us/sample - loss: 0.0541 - mse: 0.0541 - val_loss: 0.0726 - val_mse: 0.0726\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 18s 565us/sample - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 19s 601us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 19s 602us/sample - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 18s 573us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 17s 543us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 18s 576us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 19s 606us/sample - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "4-16-Epochs=30-TIME=1586825177\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 3s 101us/sample - loss: 3.7025 - mse: 3.7025 - val_loss: 0.2320 - val_mse: 0.2320\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 2s 63us/sample - loss: 0.1574 - mse: 0.1574 - val_loss: 0.1529 - val_mse: 0.1529\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.1408 - mse: 0.1408 - val_loss: 0.1493 - val_mse: 0.1493\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.1386 - mse: 0.1386 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 2s 64us/sample - loss: 0.1373 - mse: 0.1373 - val_loss: 0.1465 - val_mse: 0.1465\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 1s 37us/sample - loss: 0.1345 - mse: 0.1345 - val_loss: 0.1438 - val_mse: 0.1438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 4s 125us/sample - loss: 0.1326 - mse: 0.1326 - val_loss: 0.1425 - val_mse: 0.1425\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 64us/sample - loss: 0.1315 - mse: 0.1315 - val_loss: 0.1417 - val_mse: 0.1417\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 2s 60us/sample - loss: 0.1309 - mse: 0.1309 - val_loss: 0.1412 - val_mse: 0.1412\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.1305 - mse: 0.1305 - val_loss: 0.1409 - val_mse: 0.1409\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 2s 55us/sample - loss: 0.1303 - mse: 0.1303 - val_loss: 0.1407 - val_mse: 0.1407\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 2s 64us/sample - loss: 0.1301 - mse: 0.1301 - val_loss: 0.1406 - val_mse: 0.1406\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.1301 - mse: 0.1301 - val_loss: 0.1405 - val_mse: 0.1405\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.1300 - mse: 0.1300 - val_loss: 0.1405 - val_mse: 0.1405\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.1299 - mse: 0.1299 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.1299 - mse: 0.1299 - val_loss: 0.1405 - val_mse: 0.1405\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 61us/sample - loss: 0.1298 - mse: 0.1298 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 1s 46us/sample - loss: 0.1298 - mse: 0.1298 - val_loss: 0.1404 - val_mse: 0.1404\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 4s 139us/sample - loss: 0.1297 - mse: 0.1297 - val_loss: 0.1401 - val_mse: 0.1401\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.1296 - mse: 0.1296 - val_loss: 0.1401 - val_mse: 0.1401\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 2s 64us/sample - loss: 0.1296 - mse: 0.1296 - val_loss: 0.1401 - val_mse: 0.1401\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 2s 63us/sample - loss: 0.1296 - mse: 0.1296 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.1296 - mse: 0.1296 - val_loss: 0.1401 - val_mse: 0.1401\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 54us/sample - loss: 0.1295 - mse: 0.1295 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 2s 57us/sample - loss: 0.1295 - mse: 0.1295 - val_loss: 0.1399 - val_mse: 0.1399\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 2s 53us/sample - loss: 0.1295 - mse: 0.1295 - val_loss: 0.1399 - val_mse: 0.1399\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 2s 58us/sample - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 2s 51us/sample - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 1s 38us/sample - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "4-64-Epochs=30-TIME=1586825236\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 3s 96us/sample - loss: 1.1879 - mse: 1.1879 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 5s 144us/sample - loss: 0.1329 - mse: 0.1329 - val_loss: 0.1416 - val_mse: 0.1416\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 3s 85us/sample - loss: 0.1304 - mse: 0.1304 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 3s 89us/sample - loss: 0.1296 - mse: 0.1296 - val_loss: 0.1399 - val_mse: 0.1399\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.1293 - mse: 0.1293 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.1293 - mse: 0.1293 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 3s 88us/sample - loss: 0.1293 - mse: 0.1293 - val_loss: 0.1397 - val_mse: 0.1397\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 73us/sample - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 5s 148us/sample - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 3s 95us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1397 - val_mse: 0.1397\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 3s 96us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 85us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 3s 87us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 3s 89us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 72us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 2s 80us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1397 - val_mse: 0.1397\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 2s 63us/sample - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 5s 156us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 3s 94us/sample - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 103us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 3s 91us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 3s 82us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 3s 80us/sample - loss: 0.1293 - mse: 0.1293 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 2s 66us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 2s 52us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 1s 39us/sample - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 1s 47us/sample - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1406 - val_mse: 0.1406\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 5s 156us/sample - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 92us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1394 - val_mse: 0.1394\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 3s 104us/sample - loss: 0.1290 - mse: 0.1290 - val_loss: 0.1397 - val_mse: 0.1397\n",
      "4-128-Epochs=30-TIME=1586825321\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 4s 138us/sample - loss: 1.0108 - mse: 1.0108 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 3s 107us/sample - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0164 - val_mse: 0.0164\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 2s 71us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 2s 56us/sample - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 1s 47us/sample - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 1s 48us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 2s 49us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 6s 177us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 3s 109us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 3s 102us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 3s 101us/sample - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 2s 75us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 5s 154us/sample - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 3s 109us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 4s 113us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 3s 103us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 3s 94us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 3s 95us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0247 - val_mse: 0.0247\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 3s 85us/sample - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 2s 62us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 6s 180us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 3s 97us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 4s 116us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 4s 114us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 3s 88us/sample - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 3s 84us/sample - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 2s 76us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "4-512-Epochs=30-TIME=1586825413\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n",
      "31211/31211 [==============================] - 11s 353us/sample - loss: 0.0996 - mse: 0.0996 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 8s 271us/sample - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0419 - val_mse: 0.0419\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 8s 259us/sample - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 9s 299us/sample - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 8s 249us/sample - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 10s 307us/sample - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 8s 271us/sample - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 8s 251us/sample - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 10s 327us/sample - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 9s 273us/sample - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 8s 244us/sample - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 11s 358us/sample - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 8s 262us/sample - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 9s 281us/sample - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0205 - val_mse: 0.0205\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 9s 293us/sample - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 8s 264us/sample - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 10s 336us/sample - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 9s 284us/sample - loss: 0.0667 - mse: 0.0667 - val_loss: 0.0282 - val_mse: 0.0282\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 7s 229us/sample - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 11s 362us/sample - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0181 - val_mse: 0.0181\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 9s 282us/sample - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 10s 322us/sample - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 9s 272us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 8s 243us/sample - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0622 - val_mse: 0.0622\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 10s 327us/sample - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0188 - val_mse: 0.0188\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 9s 293us/sample - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 7s 212us/sample - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 11s 356us/sample - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 8s 268us/sample - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0193 - val_mse: 0.0193\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 10s 306us/sample - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "4-1024-Epochs=30-TIME=1586825684\n",
      "Train on 31211 samples, validate on 7803 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 27s 849us/sample - loss: 0.1041 - mse: 0.1041 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 2/30\n",
      "31211/31211 [==============================] - 26s 833us/sample - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 3/30\n",
      "31211/31211 [==============================] - 23s 743us/sample - loss: 0.0511 - mse: 0.0511 - val_loss: 0.9483 - val_mse: 0.9483\n",
      "Epoch 4/30\n",
      "31211/31211 [==============================] - 25s 788us/sample - loss: 0.1167 - mse: 0.1167 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 5/30\n",
      "31211/31211 [==============================] - 24s 762us/sample - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0281 - val_mse: 0.0281\n",
      "Epoch 6/30\n",
      "31211/31211 [==============================] - 26s 820us/sample - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 7/30\n",
      "31211/31211 [==============================] - 25s 810us/sample - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 8/30\n",
      "31211/31211 [==============================] - 25s 802us/sample - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 9/30\n",
      "31211/31211 [==============================] - 25s 801us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 10/30\n",
      "31211/31211 [==============================] - 25s 788us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 11/30\n",
      "31211/31211 [==============================] - 22s 701us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 12/30\n",
      "31211/31211 [==============================] - 25s 800us/sample - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 13/30\n",
      "31211/31211 [==============================] - 24s 775us/sample - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 14/30\n",
      "31211/31211 [==============================] - 25s 798us/sample - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 15/30\n",
      "31211/31211 [==============================] - 25s 813us/sample - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0427 - val_mse: 0.0427\n",
      "Epoch 16/30\n",
      "31211/31211 [==============================] - 25s 809us/sample - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 17/30\n",
      "31211/31211 [==============================] - 26s 820us/sample - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 18/30\n",
      "31211/31211 [==============================] - 24s 774us/sample - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 19/30\n",
      "31211/31211 [==============================] - 25s 808us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 20/30\n",
      "31211/31211 [==============================] - 23s 749us/sample - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 21/30\n",
      "31211/31211 [==============================] - 26s 830us/sample - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 22/30\n",
      "31211/31211 [==============================] - 24s 769us/sample - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 23/30\n",
      "31211/31211 [==============================] - 25s 795us/sample - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 24/30\n",
      "31211/31211 [==============================] - 28s 883us/sample - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 25/30\n",
      "31211/31211 [==============================] - 25s 813us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 26/30\n",
      "31211/31211 [==============================] - 26s 824us/sample - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 27/30\n",
      "31211/31211 [==============================] - 26s 827us/sample - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 28/30\n",
      "31211/31211 [==============================] - 25s 802us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 29/30\n",
      "31211/31211 [==============================] - 26s 821us/sample - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 30/30\n",
      "31211/31211 [==============================] - 25s 816us/sample - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0163 - val_mse: 0.0163\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# writer = pd.ExcelWriter('output/results_AE_test1.xlsx', engine='xlsxwriter')\n",
    "\n",
    "NUM_LAYERS = [1,2,3,4]\n",
    "NUM_BASE_NODES = [16,64,128,512,1024]\n",
    "\n",
    "for numLayers in NUM_LAYERS:\n",
    "    for numNodes in NUM_BASE_NODES:\n",
    "        LOGNAME = \"{}-{}-Epochs={}-TIME={}\".format(numLayers, numNodes , EPOCHS, int(time.time()) )\n",
    "        print(LOGNAME)\n",
    "        tensorboard = TensorBoard(log_dir='logs/AE/{}'.format(LOGNAME))\n",
    "        ae_model = build_autoencoder(X, num_hidden_layers = numLayers, num_nodes = numNodes)\n",
    "        ae_model.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE, callbacks = [tensorboard],\n",
    "                    validation_split=0.2)\n",
    "\n",
    "#         result = compareClasifiers (ae_model, X_train, y_train, X_test, y_test)\n",
    "#         sheetName = '{}-{}'.format(numLayers, numNodes)\n",
    "#         result.to_excel(writer, sheet_name = sheetName)\n",
    "                    \n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs of each architecture are recorded using the tensorboard callback and can be accessed by running the below shell command and accessing the provided link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir logs/AE/test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of test 1 reveal that too many layers and nodes decreased the performance of the autoencoder. Most of the models tested had similar performance when looking at the loss and mse curves on tensorboard. As such, the model was chosen based on computational efficiency and smoothness of the curves.  \n",
    "\n",
    "The model chosen has 2 hidden layers with a base number of nodes to 16. The number of layers here refers to the number of layers in the encoder, which match the number of layers in the decoder due to symmetry. This means that the actual model has 4 hidden layers of 16-32-32-16 nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test 2: Larger Networks\n",
    "The first test of autoencoder models reveals that larger networks are neccessary and they must be trained for more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_LAYERS = [2,3,4,5,6]\n",
    "NUM_BASE_NODES = [64,128,256,512,1024]\n",
    "ACTS = ['tanh','relu']\n",
    "\n",
    "\n",
    "for numLayers in NUM_LAYERS:\n",
    "    for numNodes in NUM_BASE_NODES:\n",
    "        for ac in ACTS:\n",
    "            LOGNAME = \"{}-{}-Epochs={}-TIME={}\".format(numLayers, numNodes , EPOCHS, int(time.time()) )\n",
    "            print(LOGNAME)\n",
    "            tensorboard = TensorBoard(log_dir='logs/AE/{}/{}'.format(ac,LOGNAME))\n",
    "            ae_model = build_autoencoder(X, num_hidden_layers = numLayers, base_nodes = numNodes, act = ac)\n",
    "            ae_model.fit(X, X, epochs = EPOCHS, batch_size = BATCH_SIZE, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test 3: Batch Sizes and Epochs\n",
    "Now we will test for different batch sizes, and we will use a larger number of epochs to see if performance can improve. To save time, we can use an early stop callback which stops the training when a specific metric reaches a specific goal. In this case, we can stop the training if the MSE does not drop for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='mse',\n",
    "                                                      min_delta=0.0001,\n",
    "                                                      patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The model chosen from test 1:\n",
    "NUM_LAYERS = 2\n",
    "NUM_NODES = 16\n",
    "\n",
    "BATCH_SIZES = [8,16,32,64,128,256,512,1024,2048]\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "for batchSize in BATCH_SIZES:\n",
    "    LOGNAME = \"Batch={}--TIME={}\".format(batchSize , int(time.time()) )\n",
    "    print(LOGNAME)\n",
    "    tensorboard = TensorBoard(log_dir='logs/AE/test2/{}'.format(LOGNAME))\n",
    "    ae_model = build_autoencoder(X, num_hidden_layers = NUM_LAYERS, base_nodes = NUM_NODES)\n",
    "    ae_model.fit(X_train, X_train, \n",
    "        epochs = EPOCHS,\n",
    "        batch_size = batchSize,\n",
    "                callbacks = [tensorboard, earlystop_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the test results by running the below shell command. These results show that a batch size of 1024 had the best performance, measured by the MSE, smoothness of the loss curve, and the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir logs/AE/test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test 3: Retesting different architectures with classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Combining Autoencoder with Classifiers\n",
    "Now that we have an autoencoder with tuned parameters, we can test the effectiveness of this autoencoder by observing the MSE of different classifiers with and without the autoencoder. The top three classifiers, determined and tuned in the previous notebook, are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest_final = RandomForestRegressor(n_estimators=50)\n",
    "ridge_final = Ridge(alpha=5)\n",
    "huber_final = HuberRegressor(alpha=10, epsilon=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we re-define the final autoencoder model, train it, and get the new representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGNAME = \"FinalAE-TIME={}\".format(batchSize , int(time.time()) )\n",
    "tensorboard = TensorBoard(log_dir='logs/final/{}'.format(LOGNAME))\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "NUM_NODES = 16\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 500\n",
    "\n",
    "ae_model = build_autoencoder(X, num_hidden_layers = NUM_LAYERS, base_nodes = NUM_NODES)\n",
    "ae_model.fit(X_train, X_train, \n",
    "             epochs = EPOCHS,\n",
    "             batch_size = BATCH_SIZE,\n",
    "             callbacks = [tensorboard, earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New representations:\n",
    "X_train2 = ae_model.predict(X_train)\n",
    "X_test2 = ae_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "randomForest_final.fit(X_train,y_train)\n",
    "rf_y = randomForest_final.predict(X_test)\n",
    "rf_result = mean_squared_error(y_test, rf_y)\n",
    "\n",
    "randomForest_final.fit(X_train2,y_train)\n",
    "rf_y2 = randomForest_final.predict(X_test2)\n",
    "rf_result2 = mean_squared_error(y_test, rf_y2)\n",
    "\n",
    "print(\"Random Forest: {}\".format(rf_result))\n",
    "print(\"Random Forest with Autoencoder: {}\".format(rf_result2))\n",
    "\n",
    "\n",
    "# Ridge\n",
    "ridge_final.fit(X_train,y_train)\n",
    "ridge_y = ridge_final.predict(X_test)\n",
    "ridge_result = mean_squared_error(y_test, ridge_y)\n",
    "\n",
    "ridge_final.fit(X_train2,y_train)\n",
    "ridge_y2 = ridge_final.predict(X_test2)\n",
    "ridge_result2 = mean_squared_error(y_test, ridge_y2)\n",
    "\n",
    "print(\"Ridge : {}\".format(ridge_result))\n",
    "print(\"Ridge with Autoencoder: {}\".format(ridge_result2))\n",
    "\n",
    "# Huber\n",
    "huber_final.fit(X_train,y_train)\n",
    "huber_y = huber_final.predict(X_test)\n",
    "huber_result = mean_squared_error(y_test,huber_y)\n",
    "\n",
    "huber_final.fit(X_train2,y_train)\n",
    "huber_y2 = huber_final.predict(X_test2)\n",
    "huber_result2 = mean_squared_error(y_test,huber_y2)\n",
    "\n",
    "print(\"Huber: {}\".format(huber_result))\n",
    "print(\"Huber with Autoencoder: {}\".format(huber_result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb-env",
   "language": "python",
   "name": "airbnb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
