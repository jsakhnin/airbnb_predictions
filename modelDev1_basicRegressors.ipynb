{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-50dd7c287d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ElasticNet,  HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data\n",
    "The data being loaded below has already been processed in the data processing notebook of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('input/processed_data_nyc.csv', index_col = 0)\n",
    "numerical_data = pd.read_csv('input/processed_data_nyc_numerical.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are predicting price in this notebook, we will take price as the label (y vector) and we will drop it from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.price\n",
    "data = data.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn the data and label to numpy arrays, which is more convenient for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(data)\n",
    "y = np.asarray(y).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model, we need to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training Dataset: {}\".format(X_train.shape))\n",
    "print(\"Testing Dataset: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better results, data should be scaled to a zero mean and unit variance. This can be done using the robust scaler from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Models\n",
    "Before we begin testing different models, we will declare a general function for cross-validation. I will use k-fold validation with 5 folds evaluated based on the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmse_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_data)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a basic cross-validation test on some well-known regression models without any fine-tuning. We want to get an idea of what's the best model to invest our time in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [LinearRegression, Ridge, Lasso, ElasticNet, RandomForestRegressor, HuberRegressor]\n",
    "\n",
    "for Model in MODELS:\n",
    "    cv_res = rmse_cv(Model())\n",
    "    print('{}: {:.5f} +/- {:5f}'.format(Model.__name__, -cv_res.mean(), cv_res.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above results, the three seemingly best classifiers for this application are:  \n",
    "1) Random Forest Regressor  \n",
    "2) Ridge Regressor   \n",
    "3) Huber Regressor   \n",
    "\n",
    "As such, I will test these three models and tune their hyperparameters to see which can achieve the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forest Regression\n",
    "In Random Forest Regression, the main parameter to be hypertuned is the number of estimators, which is the number of decision trees used in this ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_estimators = [2, 5, 10, 20, 50, 100, 200, 300]\n",
    "\n",
    "cv_randomForest = []\n",
    "cv_randomForest_time = []\n",
    "counter = 0\n",
    "for n in num_estimators:\n",
    "    time1 = time.time()\n",
    "    cv_temp = -rmse_cv(RandomForestRegressor(n_estimators = n)).mean()\n",
    "    cv_randomForest.append(cv_temp)\n",
    "    time2 = time.time()\n",
    "    timeF = time2- time1\n",
    "    print(\"Number of estimators {}: MSE = {}, Time = {}\".format(n, cv_randomForest[counter], timeF))\n",
    "    cv_randomForest_time.append(timeF)\n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the cross-validated mean squared error (MSE) for each number of estimators tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_randomForest = pd.Series(cv_randomForest, index = num_estimators)\n",
    "cv_randomForest_time = pd.Series(cv_randomForest_time, index = num_estimators)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(21, 8))\n",
    "\n",
    "cv_randomForest.plot(title = \"Random Forest RMSE\", style = '-+', ax = axes[0])\n",
    "axes[0].set_xlabel(\"Number of Estimators\") \n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "\n",
    "cv_randomForest_time.plot(title = \"Random Forest Training Time\", style = '-+', ax = axes[1])\n",
    "axes[1].set_xlabel(\"Number of Estimators\") \n",
    "axes[1].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "plt.savefig('output/figures/random_forest_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, we can see that increasing the number of estimators improves the performance of the model. However, this comes at the cost of increased computational burden. Higher number of estimators also means longer training time. So we need to pick a good trade off point where we can achieve computational efficiency. The change in MSE between 50 and 300 is very minor but the change in computational burden (measured through training time) is quite large. For that reason, the number of estimators chosen will be 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ridge Regression\n",
    "Ridge regression has two main parameters to tune. The first is the solver used to train the regression model, and the regularization strength, alpha, which reduces the variance of the estimates. In the scikit-learn library we are using, the solver is set to auto by default. This means that the algorithm will choose the solver based on the nature of the data automatically. As such, we will only cross validate for alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 0.3, 0.5, 1, 1.5, 2, 5, 10, 20, 50, 100]\n",
    "\n",
    "cv_ridge = []\n",
    "cv_ridge_time = []\n",
    "counter = 0\n",
    "for a in alphas:\n",
    "    time1 = time.time()\n",
    "    cv_temp = -rmse_cv(Ridge(alpha = a)).mean()\n",
    "    cv_ridge.append(cv_temp)\n",
    "    time2 = time.time()\n",
    "    timeF = time2- time1\n",
    "    print(\"Alpha {}: MSE = {}, Time = {}\".format(a, cv_ridge[counter], timeF))\n",
    "    cv_ridge_time.append(timeF)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge_time = pd.Series(cv_ridge_time, index = alphas)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(21, 8))\n",
    "\n",
    "cv_ridge.plot(title = \"Ridge RMSE\", style = '-+', ax = axes[0])\n",
    "axes[0].set_xlabel(\"Alpha\") \n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "\n",
    "cv_ridge_time.plot(title = \"Ridge Training Time\", style = '-+', ax = axes[1])\n",
    "axes[1].set_xlabel(\"Alpha\") \n",
    "axes[1].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "plt.savefig('output/figures/ridge_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, changing the parameters of the ridge regressor had negligible impact on the performance. We will choose alpha of 5 due to best performance with no compromise on training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Huber Regression\n",
    "Huber Regressors have two primary parameters, epsilon and alpha. Alpha is the regularization parameter and epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers. Epsilon is a value greater than 1. We will test for both of these parameters using gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha':[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100], \n",
    "              'epsilon':[1, 1.1, 1.2,1.25, 1.3 , 1.35, 1.5, 1.75, 2, 2.5, 3]}\n",
    "huberModel = HuberRegressor()\n",
    "\n",
    "clf = GridSearchCV(huberModel, parameters, verbose=5, scoring='neg_mean_squared_error')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the mean test scores above, we can clearly see that there is very little difference in performance from changing the hyper-parameters. Now, let's look at the difference in training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.cv_results_['mean_fit_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the difference in training time and score are both negligible, we will pick the parameters with the highest score in this gridsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Final Tests\n",
    "In this notebook, we explored the most popular regression models, and then tested and tuned three different methods. The scoring method used the mean squared error. The final parameters chosen for each model are:  \n",
    "\n",
    "1) Random Forest: Number of estimators = 50  \n",
    "2) Ridge Regression: Alpha = 5  \n",
    "3) Huber Regression: Alpha = 10, Epsilon = 3  \n",
    "\n",
    "As such, we will define our final models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest_final = RandomForestRegressor(n_estimators=50)\n",
    "ridge_final = Ridge(alpha=5)\n",
    "huber_final = HuberRegressor(alpha=10, epsilon=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train each of those models on the entire training set, and test it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "randomForest_final.fit(X_train,y_train)\n",
    "rf_y = randomForest_final.predict(X_test)\n",
    "rf_result = mean_squared_error(y_test, rf_y)\n",
    "print(\"Random Forest: {}\".format(rf_result))\n",
    "\n",
    "# Ridge\n",
    "ridge_final.fit(X_train,y_train)\n",
    "ridge_y = ridge_final.predict(X_test)\n",
    "ridge_result = mean_squared_error(y_test, ridge_y)\n",
    "\n",
    "print(\"Ridge : {}\".format(ridge_result))\n",
    "\n",
    "# Huber\n",
    "huber_final.fit(X_train,y_train)\n",
    "huber_y = huber_final.predict(X_test)\n",
    "huber_result = mean_squared_error(y_test,huber_y)\n",
    "print(\"Huber: {}\".format(huber_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Saving Results\n",
    "Now we will save the data for future use so we do not have to run this notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Random Forest', 'Ridge', 'Huber']\n",
    "parameters = ['N_Estimators = 50', 'Alpha = 5', 'Alpha = 10, Epsilon = 3']\n",
    "final_mse = [rf_result, ridge_result, huber_result]\n",
    "\n",
    "finalResult_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Parameters': parameters,\n",
    "    'MSE': final_mse,\n",
    "})\n",
    "\n",
    "finalResult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest_df = pd.concat([cv_randomForest, cv_randomForest_time], axis=1)\n",
    "randomForest_df.columns = ['MSE', 'Time']\n",
    "\n",
    "ridge_df = pd.concat([cv_ridge, cv_ridge_time], axis=1)\n",
    "ridge_df.columns = ['MSE', 'Time']\n",
    "\n",
    "huber_df = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('output/results_cv.xlsx', engine='xlsxwriter')\n",
    "randomForest_df.to_excel(writer, sheet_name='Random Forest')\n",
    "ridge_df.to_excel(writer, sheet_name='Ridge')\n",
    "huber_df.to_excel(writer, sheet_name='Huber')\n",
    "finalResult_df.to_excel(writer, sheet_name='Final Models Results')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
